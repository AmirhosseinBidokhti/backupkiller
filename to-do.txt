
# GENERATING A LIST OF POTENTIAL BACKUP FILES FROM A GIVE INPUT (FILE/STDIN) OF URLs 
-------------------------------------------------------------------------------------

- URLs (different cases)

	domain
	domain + path
	domain + path + params
	domain + params
	
	with/without schemas
	
		https://
		http://
		https://www.
		http://www.

------------------------------------------------------------------------------------

- A wordlist of the backup exentsions and more
	file
	json
	dynamic 


// Things to conisder

1. URL/URL.{backup_word}

2. URL.{backup_word}

3.
$ python backupkiller.py -u https://domain.tld | grep "7z"
https://domain.tld/www.domain.7z
https://domain.tld/www.domain.tld.7z
https://domain.tld/domain.7z       
https://domain.tld/domain.tld.7z   


TODOS:
// Trying different variations (with and without www)
- www.URL.{word}
- www.URL.tld.{word}
- {no www}.URL.{word}
- {no www}.URL.tld.{word}

- this might cause duplicates since we are giving a file of urls that might not be the same (?)
/ an array to track the already added domains ?


// if no path than drop (no point in doing domain.tld.{word} since it become another tld)

// make reusable function for URL PATH etc stuffs

// Properly handle if the stdin or file is not found.

// "/" in path that is always present handling this


// when the scheme is not there (domain.tld) then it fucks up the path validation
		if u.hostname is length of zero i think will solve this

// About .git or other stuffs that might be present in the root directory
	e.g. domain.tld/.git
	what to do about them?
	should i test for domain.tld/.{words} ???


// Should i also test for domain.tld/domain.tld/path.{word}


// Deduplicate the input urls first (use Deduplicate)

